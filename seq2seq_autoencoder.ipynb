{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "layers = [35, 35] # Number of hidden neuros in each layer of the encoder and decoder\n",
    "\n",
    "learning_rate = 0.01\n",
    "decay = 0 # Learning rate decay\n",
    "optimiser = keras.optimizers.Adam(lr=learning_rate, decay=decay) # Other possible optimiser \"sgd\" (Stochastic Gradient Descent)\n",
    "\n",
    "num_input_features = 1 # The dimensionality of the input at each time step. In this case a 1D signal.\n",
    "num_output_features = 1 # The dimensionality of the output at each time step. In this case a 1D signal.\n",
    "# There is no reason for the input sequence to be of same dimension as the ouput sequence.\n",
    "# For instance, using 3 input signals: consumer confidence, inflation and house prices to predict the future house prices.\n",
    "\n",
    "loss = \"mse\" # Other loss functions are possible, see Keras documentation.\n",
    "\n",
    "# Regularisation isn't really needed for this application\n",
    "lambda_regulariser = 0.000001 # Will not be used if regulariser is None\n",
    "regulariser = None # Possible regulariser: keras.regularizers.l2(lambda_regulariser)\n",
    "\n",
    "batch_size = 512\n",
    "steps_per_epoch = 200 # batch_size * steps_per_epoch = total number of training examples\n",
    "epochs = 15\n",
    "\n",
    "input_sequence_length = 15 # Length of the sequence used by the encoder\n",
    "target_sequence_length = 15 # Length of the sequence predicted by the decoder\n",
    "num_steps_to_predict = 20 # Length to use when testing the model\n",
    "\n",
    "num_signals = 2 # The number of random sine waves the compose the signal. The more sine waves, the harder the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence.\n",
    "encoder_inputs = keras.layers.Input(shape=(None, num_input_features))\n",
    "\n",
    "# Create a list of RNN Cells, these are then concatenated into a single layer\n",
    "# with the RNN layer.\n",
    "encoder_cells = []\n",
    "for hidden_neurons in layers:\n",
    "    encoder_cells.append(keras.layers.GRUCell(hidden_neurons,\n",
    "                                              kernel_regularizer=regulariser,\n",
    "                                              recurrent_regularizer=regulariser,\n",
    "                                              bias_regularizer=regulariser))\n",
    "\n",
    "encoder = keras.layers.RNN(encoder_cells, return_state=True)\n",
    "\n",
    "encoder_outputs_and_states = encoder(encoder_inputs)\n",
    "\n",
    "# Discard encoder outputs and only keep the states.\n",
    "# The outputs are of no interest to us, the encoder's\n",
    "# job is to create a state describing the input sequence.\n",
    "encoder_states = encoder_outputs_and_states[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder input will be set to zero (see random_sine function of the utils module).\n",
    "# Do not worry about the input size being 1, I will explain that in the next cell.\n",
    "decoder_inputs = keras.layers.Input(shape=(None, 1))\n",
    "\n",
    "decoder_cells = []\n",
    "for hidden_neurons in layers:\n",
    "    decoder_cells.append(keras.layers.GRUCell(hidden_neurons,\n",
    "                                              kernel_regularizer=regulariser,\n",
    "                                              recurrent_regularizer=regulariser,\n",
    "                                              bias_regularizer=regulariser))\n",
    "\n",
    "decoder = keras.layers.RNN(decoder_cells, return_sequences=True, return_state=True)\n",
    "\n",
    "# Set the initial state of the decoder to be the ouput state of the encoder.\n",
    "# This is the fundamental part of the encoder-decoder.\n",
    "decoder_outputs_and_states = decoder(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# Only select the output of the decoder (not the states)\n",
    "decoder_outputs = decoder_outputs_and_states[0]\n",
    "\n",
    "# Apply a dense layer with linear activation to set output to correct dimension\n",
    "# and scale (tanh is default activation for GRU in Keras, our output sine function can be larger then 1)\n",
    "decoder_dense = keras.layers.Dense(num_output_features,\n",
    "                                   activation='linear',\n",
    "                                   kernel_regularizer=regulariser,\n",
    "                                   bias_regularizer=regulariser)\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model using the functional API provided by Keras.\n",
    "# The functional API is great, it gives an amazing amount of freedom in architecture of your NN.\n",
    "# A read worth your time: https://keras.io/getting-started/functional-api-guide/ \n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "model.compile(optimizer=optimiser, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = np.load('clf_data.npy')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "clf = scaler.fit_transform(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sine(batch_size, steps_per_epoch,\n",
    "                input_sequence_length, target_sequence_length,\n",
    "                min_frequency=0.1, max_frequency=10,\n",
    "                min_amplitude=0.1, max_amplitude=1,\n",
    "                min_offset=-0.5, max_offset=0.5,\n",
    "                num_signals=3, seed=43):\n",
    "    \"\"\"Produce a batch of signals.\n",
    "    The signals are the sum of randomly generated sine waves.\n",
    "    Arguments\n",
    "    ---------\n",
    "    batch_size: Number of signals to produce.\n",
    "    steps_per_epoch: Number of batches of size batch_size produced by the\n",
    "        generator.\n",
    "    input_sequence_length: Length of the input signals to produce.\n",
    "    target_sequence_length: Length of the target signals to produce.\n",
    "    min_frequency: Minimum frequency of the base signals that are summed.\n",
    "    max_frequency: Maximum frequency of the base signals that are summed.\n",
    "    min_amplitude: Minimum amplitude of the base signals that are summed.\n",
    "    max_amplitude: Maximum amplitude of the base signals that are summed.\n",
    "    min_offset: Minimum offset of the base signals that are summed.\n",
    "    max_offset: Maximum offset of the base signals that are summed.\n",
    "    num_signals: Number of signals that are summed together.\n",
    "    seed: The seed used for generating random numbers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    signals: 2D array of shape (batch_size, sequence_length)\n",
    "    \"\"\"\n",
    "    num_points = input_sequence_length + target_sequence_length\n",
    "    x = np.arange(num_points) * 2*np.pi/30\n",
    "\n",
    "    while True:\n",
    "        # Reset seed to obtain same sequences from epoch to epoch\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        for _ in range(steps_per_epoch):\n",
    "            signals = np.zeros((batch_size, num_points))\n",
    "            for _ in range(num_signals):\n",
    "                # Generate random amplitude, frequence, offset, phase \n",
    "                amplitude = (np.random.rand(batch_size, 1) * \n",
    "                            (max_amplitude - min_amplitude) +\n",
    "                             min_amplitude)\n",
    "                frequency = (np.random.rand(batch_size, 1) * \n",
    "                            (max_frequency - min_frequency) + \n",
    "                             min_frequency)\n",
    "                offset = (np.random.rand(batch_size, 1) * \n",
    "                         (max_offset - min_offset) + \n",
    "                          min_offset)\n",
    "                phase = np.random.rand(batch_size, 1) * 2 * np.pi \n",
    "                         \n",
    "\n",
    "                signals += amplitude * np.sin(frequency * x + phase)\n",
    "            signals = np.expand_dims(signals, axis=2)\n",
    "            \n",
    "            encoder_input = signals[:, :input_sequence_length, :]\n",
    "            decoder_output = signals[:, input_sequence_length:, :]\n",
    "            \n",
    "            # The output of the generator must be ([encoder_input, decoder_input], [decoder_output])\n",
    "            decoder_input = np.zeros((decoder_output.shape[0], decoder_output.shape[1], 1))\n",
    "            yield ([encoder_input, decoder_input], decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = random_sine(batch_size=batch_size,\n",
    "                                   steps_per_epoch=steps_per_epoch,\n",
    "                                   input_sequence_length=input_sequence_length,\n",
    "                                   target_sequence_length=target_sequence_length,\n",
    "                                   min_frequency=0.1, max_frequency=10,\n",
    "                                   min_amplitude=0.1, max_amplitude=1,\n",
    "                                   min_offset=-0.5, max_offset=0.5,\n",
    "                                   num_signals=num_signals, seed=1969)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-348d76c46974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "len(train_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 0.0873\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 0.0611\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 0.0487\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 0.0406\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 11s 55ms/step - loss: 0.0358\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 0.0325\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 0.0298\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 0.0278\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 0.0259\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 11s 56ms/step - loss: 0.0246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f7ee0699750>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_data_generator, steps_per_epoch=steps_per_epoch, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_generator = random_sine(batch_size=1000,\n",
    "                                  steps_per_epoch=steps_per_epoch,\n",
    "                                  input_sequence_length=input_sequence_length,\n",
    "                                  target_sequence_length=target_sequence_length,\n",
    "                                  min_frequency=0.1, max_frequency=10,\n",
    "                                  min_amplitude=0.1, max_amplitude=1,\n",
    "                                  min_offset=-0.5, max_offset=0.5,\n",
    "                                  num_signals=num_signals, seed=2000)\n",
    "\n",
    "(x_encoder_test, x_decoder_test), y_test = next(test_data_generator) # x_decoder_test is composed of zeros.\n",
    "\n",
    "y_test_predicted = model.predict([x_encoder_test, x_decoder_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 matlab",
   "language": "python",
   "name": "dlwp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
