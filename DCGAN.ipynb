{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "886cf794-b413-49f0-a78d-0003664abead",
    "_uuid": "ecb2ba94-3d48-4876-8200-65fa763b95ed",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L1X_zrhblRB2",
    "outputId": "917e227e-6fb9-437a-a46c-305e609bcbd4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras import initializers\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import (BatchNormalization, Conv2D, Conv2DTranspose, Dense,\n",
    "                          Dropout, Flatten, Input, Reshape, UpSampling2D,\n",
    "                          ZeroPadding2D)\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lines(array):\n",
    "#     line_start=5\n",
    "#     line_end=18\n",
    "#     array_lines=np.copy(array)\n",
    "#     for line_index in range(line_start, line_end):\n",
    "#         array_lines[line_index] = np.full_like(array[line_index], np.nan)\n",
    "#     return array_lines\n",
    "    specificLinesIndexes = np.array([2,5,6,9,10,11,14,17,19])\n",
    "    arrayLines=np.copy(array)\n",
    "    for index in specificLinesIndexes:\n",
    "        arrayLines[index] = np.full_like(array[index], np.nan)\n",
    "    return arrayLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfFiles=0\n",
    "try: \n",
    "    filesXtrain = os.listdir('../TrainingDataset/x_train/')\n",
    "    numberOfFiles = len(filesXtrain)\n",
    "except: print('File not found')\n",
    "numberOfFiles-=11\n",
    "testingSetSize = 100\n",
    "mx_train_lines = np.empty((numberOfFiles-testingSetSize,24,144))\n",
    "mx_train = np.empty((numberOfFiles-testingSetSize,24,144))\n",
    "mx_train_lines_nan = np.empty((numberOfFiles-testingSetSize,24,144))\n",
    "mx_test_lines = np.empty((testingSetSize,24,144))\n",
    "mx_test_lines_nan = np.empty((testingSetSize,24,144))\n",
    "mx_test = np.empty((testingSetSize,24,144))\n",
    "\n",
    "for i in range(0,numberOfFiles-testingSetSize):\n",
    "    mx_train[i] = np.load('../TrainingDataset/x_train/Y2_36_60_{}.npy'.format(i))\n",
    "    mx_train_lines[i] = make_lines(mx_train[i])\n",
    "    mx_train_lines_nan[i] = make_lines(mx_train[i])\n",
    "for i in range(numberOfFiles-testingSetSize,numberOfFiles):\n",
    "    mx_test[i-(numberOfFiles-testingSetSize)-1] = np.load('../TrainingDataset/x_train/Y2_36_60_{}.npy'.format(i))\n",
    "    mx_test_lines[i-(numberOfFiles-testingSetSize)-1] = make_lines(mx_test[i-(numberOfFiles-testingSetSize)-1])\n",
    "    mx_test_lines_nan[i-(numberOfFiles-testingSetSize)-1] = make_lines(mx_test[i-(numberOfFiles-testingSetSize)-1])\n",
    "\n",
    "mx_train_lines=np.nan_to_num(mx_train_lines)\n",
    "mx_train=np.nan_to_num(mx_train)\n",
    "mx_test_lines=np.nan_to_num(mx_test_lines)\n",
    "mx_test=np.nan_to_num(mx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "_cell_guid": "56a938fc-fa20-4972-9ec9-9c6769fa6faf",
    "_uuid": "26f698d5-784c-4882-b3b4-10cfcdf493e3",
    "colab": {},
    "colab_type": "code",
    "id": "cqkoXzNElRB5"
   },
   "outputs": [],
   "source": [
    "# Consistent results\n",
    "np.random.seed(1337)\n",
    "\n",
    "# The dimension of z\n",
    "noise_dim = 144*24\n",
    "\n",
    "batch_size = 50\n",
    "steps_per_epoch = np.int16(numberOfFiles/batch_size)\n",
    "epochs = 5\n",
    "\n",
    "save_path = '../GAN/DCGAN/images/'\n",
    "\n",
    "img_rows, img_cols, channels = 144, 24, 1\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "98a90dbb-55a7-4bf1-acd2-096ec9adf718",
    "_uuid": "d1eca82d-ba97-4d46-b86d-485f2d86a62c",
    "colab": {},
    "colab_type": "code",
    "id": "k1bONiz8lRB8"
   },
   "outputs": [],
   "source": [
    "# Create path for saving images\n",
    "if save_path != None and not os.path.isdir(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "_cell_guid": "8cac0a57-f50d-4f2e-a276-5c742b5a74cc",
    "_uuid": "245e234f-0e99-4a17-8267-24b28f504306",
    "colab": {},
    "colab_type": "code",
    "id": "uEF5BN8flRCB"
   },
   "outputs": [],
   "source": [
    "def create_generator():\n",
    "    generator = Sequential()\n",
    "    \n",
    "    # Starting size\n",
    "    d = 4\n",
    "    generator.add(Dense(3*18*256, kernel_initializer=RandomNormal(0, 0.02), input_dim=noise_dim))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    # 4x4x256\n",
    "    generator.add(Reshape((3, 18, 256)))\n",
    "    \n",
    "    # 8x8x128\n",
    "    generator.add(Conv2DTranspose(128, (4, 4), strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # 16x16*128\n",
    "    generator.add(Conv2DTranspose(128, (4, 4), strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # 32x32x128\n",
    "    generator.add(Conv2DTranspose(128, (4, 4), strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    # 32x32x3\n",
    "    generator.add(Conv2D(channels, (3, 3), padding='same', activation='tanh', kernel_initializer=RandomNormal(0, 0.02)))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "5cfad0f4-6066-40c5-82a1-23c6fe0953cf",
    "_uuid": "9bdbbbb0-33dc-49d2-9dd2-127a96909ad6",
    "colab": {},
    "colab_type": "code",
    "id": "UplAGet9lRCD"
   },
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    discriminator = Sequential()\n",
    "    \n",
    "    discriminator.add(Conv2D(64, (3, 3), padding='same', kernel_initializer=RandomNormal(0, 0.02), input_shape=(img_cols, img_rows, channels)))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Conv2D(128, (3, 3), strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Conv2D(128, (3, 3), strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Conv2D(256, (3, 3), strides=2, padding='same', kernel_initializer=RandomNormal(0, 0.02)))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Flatten())\n",
    "    discriminator.add(Dropout(0.4))\n",
    "    discriminator.add(Dense(1, activation='sigmoid', input_shape=(img_cols, img_rows, channels)))\n",
    "    \n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "_cell_guid": "ae02d478-086e-4ebb-a785-3a82b20932c1",
    "_uuid": "86e3c88d-614c-4c03-b332-3331d4fd8651",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "KBjg3vMIlRCF",
    "outputId": "6b9ac932-3951-4956-e769-ce3a8f58381c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discriminator = create_discriminator()\n",
    "generator = create_generator()\n",
    "\n",
    "# Make the discriminator untrainable when we are training the generator.  This doesn't effect the discriminator by itself\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Link the two models to create the GAN\n",
    "gan_input = Input(shape=(noise_dim,))\n",
    "fake_image = generator(gan_input)\n",
    "\n",
    "gan_output = discriminator(fake_image)\n",
    "\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "_cell_guid": "d5e891f3-a40b-4796-a8fa-4a53acd8af54",
    "_uuid": "3a188e82-2d2d-4d31-87f5-9798bd5fbd1b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "myT68aLJlRCJ",
    "outputId": "d0c7702a-8969-46e2-c1ad-09e05fdb0ea8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Discriminator Loss: 0.45180705189704895 \t\t Generator Loss: 2.311074733734131\n",
      "Epoch: 1 \t Discriminator Loss: 0.48293232917785645 \t\t Generator Loss: 1.5575053691864014\n",
      "Epoch: 2 \t Discriminator Loss: 0.6606431603431702 \t\t Generator Loss: 1.0730115175247192\n",
      "Epoch: 3 \t Discriminator Loss: 0.6875193119049072 \t\t Generator Loss: 0.9164425134658813\n",
      "Epoch: 4 \t Discriminator Loss: 0.6992388963699341 \t\t Generator Loss: 1.155087947845459\n",
      "Epoch: 5 \t Discriminator Loss: 0.6691053509712219 \t\t Generator Loss: 0.8490893840789795\n",
      "Epoch: 6 \t Discriminator Loss: 0.6834601759910583 \t\t Generator Loss: 0.7793903350830078\n",
      "Epoch: 7 \t Discriminator Loss: 0.7032247185707092 \t\t Generator Loss: 0.6595121622085571\n",
      "Epoch: 8 \t Discriminator Loss: 0.7002910375595093 \t\t Generator Loss: 1.0083088874816895\n",
      "Epoch: 9 \t Discriminator Loss: 0.6649432182312012 \t\t Generator Loss: 0.8457320332527161\n",
      "Epoch: 10 \t Discriminator Loss: 0.6563476324081421 \t\t Generator Loss: 0.8764636516571045\n",
      "Epoch: 11 \t Discriminator Loss: 0.6629358530044556 \t\t Generator Loss: 0.8377701640129089\n",
      "Epoch: 12 \t Discriminator Loss: 0.6643450856208801 \t\t Generator Loss: 0.887928307056427\n",
      "Epoch: 13 \t Discriminator Loss: 0.6630600690841675 \t\t Generator Loss: 0.8425763845443726\n",
      "Epoch: 14 \t Discriminator Loss: 0.6782705187797546 \t\t Generator Loss: 0.7357074618339539\n",
      "Epoch: 15 \t Discriminator Loss: 0.6435851454734802 \t\t Generator Loss: 1.037976861000061\n",
      "Epoch: 16 \t Discriminator Loss: 0.652883768081665 \t\t Generator Loss: 1.0101979970932007\n",
      "Epoch: 17 \t Discriminator Loss: 0.6210273504257202 \t\t Generator Loss: 1.0095064640045166\n",
      "Epoch: 18 \t Discriminator Loss: 0.6380165815353394 \t\t Generator Loss: 1.0353864431381226\n",
      "Epoch: 19 \t Discriminator Loss: 0.6515364050865173 \t\t Generator Loss: 0.8933773636817932\n"
     ]
    }
   ],
   "source": [
    "# Constant noise for viewing how the GAN progresses\n",
    "static_noise = np.random.normal(0, 1, size=(10,noise_dim))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    for batch in range(steps_per_epoch):\n",
    "        noise = np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
    "        real_x = mx_train[np.random.randint(0, mx_train.shape[0], size=batch_size)]\n",
    "        real_x = np.expand_dims(real_x, 3)\n",
    "        fake_x = generator.predict(noise)\n",
    "\n",
    "        x = np.concatenate((real_x, fake_x))\n",
    "\n",
    "        disc_y = np.zeros(x.shape[0])\n",
    "        separator = np.int16(x.shape[0]/2)\n",
    "        disc_y[:separator] = 0.9\n",
    "\n",
    "        d_loss = discriminator.train_on_batch(x, disc_y)\n",
    "\n",
    "        y_gen = np.ones(batch_size)\n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "    print(f'Epoch: {epoch} \\t Discriminator Loss: {d_loss} \\t\\t Generator Loss: {g_loss}')\n",
    "    np.save(\"../GAN/DCGAN/images/DCGAN_prediction_{}\".format(epoch),generator.predict(static_noise).reshape(10,24,144))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "GANs Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
